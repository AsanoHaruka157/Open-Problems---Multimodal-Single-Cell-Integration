{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f85063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 核心库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 进度条\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 全局配置 ---\n",
    "# 设备配置，优先使用GPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 为了结果可复现，设置随机种子\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed .h5ad files...\n",
      "\n",
      "Data shapes for model training:\n",
      "train_multi_X: (100646, 160259)\n",
      "train_multi_y: (100646, 23418)\n",
      "test_multi_X: (55935, 160259) (placeholder data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# --- 数据路径 ---\n",
    "# 这是我们之前数据清洗步骤生成的文件的路径\n",
    "PATH_TRAIN_INP = \"train_multi_inputs.h5ad\"\n",
    "PATH_TRAIN_TGT = \"train_multi_targets.h5ad\"\n",
    "PATH_TEST_INP  = \"test_multi_inputs.h5ad\" # 假设测试集也已处理\n",
    "PATH_META      = \"metadata.csv\"\n",
    "\n",
    "# 加载已清洗的训练数据\n",
    "print(\"Loading pre-processed .h5ad files...\")\n",
    "adata_train_inp = ad.read_h5ad(PATH_TRAIN_INP)\n",
    "adata_train_tgt = ad.read_h5ad(PATH_TRAIN_TGT)\n",
    "adata_test_inp = ad.read_h5ad(PATH_TEST_INP) # 如果需要加载测试集\n",
    "\n",
    "# --- 关键步骤：数据对齐 ---\n",
    "# 确保输入和目标的细胞顺序完全一致\n",
    "assert np.all(adata_train_inp.obs_names == adata_train_tgt.obs_names), \"Error: Train inputs and targets have different cell orders!\"\n",
    "\n",
    "# 加载元数据并对齐到训练数据的顺序\n",
    "meta_df = pd.read_csv(PATH_META).set_index(\"cell_id\")\n",
    "meta_df = meta_df.loc[adata_train_inp.obs_names]\n",
    "\n",
    "# 保持稀疏格式以避免MemoryError\n",
    "train_multi_X = adata_train_inp.X\n",
    "train_multi_y = adata_train_tgt.X.toarray() if issparse(adata_train_tgt.X) else adata_train_tgt.X\n",
    "\n",
    "# 加载测试集数据\n",
    "test_multi_X = adata_test_inp.X\n",
    "test_df = pd.DataFrame(index=adata_test_inp.obs_names)\n",
    "\n",
    "# 打印数据维度以确认\n",
    "print(\"\\nData shapes for model training:\")\n",
    "print(f\"train_multi_X: {train_multi_X.shape}\")\n",
    "print(f\"train_multi_y: {train_multi_y.shape}\")\n",
    "print(f\"test_multi_X: {test_multi_X.shape} (placeholder data)\")\n",
    "\n",
    "# 清理内存\n",
    "del adata_train_inp, adata_train_tgt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e86af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 评估与数据处理工具 ---\n",
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"逐行计算皮尔逊相关系数并返回平均值\"\"\"\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)\n",
    "\n",
    "def zscore(x):\n",
    "    \"\"\"对输入矩阵的每一行进行Z-score标准化\"\"\"\n",
    "    x_zscore = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_row = x[i]\n",
    "        mean = np.mean(x_row)\n",
    "        std = np.std(x_row)\n",
    "        if std == 0: # 防止除以零\n",
    "            x_zscore.append(x_row - mean)\n",
    "        else:\n",
    "            x_zscore.append((x_row - mean) / std)\n",
    "    return np.array(x_zscore)\n",
    "\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    \"\"\"皮尔逊相关性损失的PyTorch实现\"\"\"\n",
    "    # 1. 中心化\n",
    "    y_true_centered = y_true - torch.mean(y_true, dim=1, keepdim=True)\n",
    "    y_pred_centered = y_pred - torch.mean(y_pred, dim=1, keepdim=True)\n",
    "    \n",
    "    # 2. L2标准化\n",
    "    y_true_norm = torch.nn.functional.normalize(y_true_centered, p=2, dim=1)\n",
    "    y_pred_norm = torch.nn.functional.normalize(y_pred_centered, p=2, dim=1)\n",
    "    \n",
    "    # 3. 计算余弦相似度并取反作为损失\n",
    "    # .mean()聚合batch中的所有样本损失\n",
    "    return -torch.nn.CosineSimilarity(dim=1)(y_true_norm, y_pred_norm).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse # Make sure issparse is imported\n",
    "\n",
    "class SingleCellDataset(Dataset):\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.is_train = targets is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        # 【修正】: 使用 .shape[0] 来获取稀疏矩阵的行数（样本数）\n",
    "        # [FIX]: Use .shape[0] to get the number of rows (samples) from the sparse matrix\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This part, which handles slicing, is correct from our previous fix.\n",
    "        feature = self.features[idx]\n",
    "        if issparse(feature):\n",
    "            feature = feature.toarray().squeeze()\n",
    "        \n",
    "        feature = torch.tensor(feature, dtype=torch.float32)\n",
    "        \n",
    "        if self.is_train:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return feature, target\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, seq_len=16, d_model=256, nhead=8, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        \"\"\"\n",
    "        num_features: 原始输入特征数 (例如: 20000+)\n",
    "        num_targets: 目标输出数 (23418)\n",
    "        seq_len: 我们要将原始特征向量“重塑”成的序列长度\n",
    "        d_model: Transformer内部的工作维度 (必须能被nhead整除)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. 线性投射层：将原始的高维特征投射到一个可以被重塑为序列的空间\n",
    "        self.projector = nn.Linear(num_features, seq_len * d_model)\n",
    "        \n",
    "        # 2. CLS Token: 类似于BERT，我们添加一个特殊的可学习的token，用于聚合整个序列的信息\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        \n",
    "        # 3. 位置编码：让模型知道序列中每个元素的位置信息\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, seq_len + 1, d_model))\n",
    "        \n",
    "        # 4. 标准的Transformer编码器\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 5. 预测头：一个简单的MLP，接收CLS Token的最终输出，并预测140个蛋白质\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_targets)\n",
    "        )\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_features)\n",
    "        \n",
    "        # 1. 投射并重塑为序列\n",
    "        x = self.projector(x) # -> (batch_size, seq_len * d_model)\n",
    "        x = x.reshape(-1, self.seq_len, self.d_model) # -> (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 2. 添加CLS token\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # -> (batch_size, 1, d_model)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # -> (batch_size, seq_len + 1, d_model)\n",
    "        \n",
    "        # 3. 添加位置编码\n",
    "        x += self.pos_encoder\n",
    "        \n",
    "        # 4. 通过Transformer编码器\n",
    "        x = self.transformer_encoder(x) # -> (batch_size, seq_len + 1, d_model)\n",
    "        \n",
    "        # 5. 取出CLS token的输出，并通过预测头\n",
    "        cls_output = x[:, 0] # -> (batch_size, d_model)\n",
    "        output = self.mlp_head(cls_output) # -> (batch_size, num_targets)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a31562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm # 引入tqdm来显示进度条\n",
    "\n",
    "def train_and_evaluate(\n",
    "    model_class,\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    folds,\n",
    "    model_params,\n",
    "    train_params,\n",
    "    loss_fn\n",
    "):\n",
    "    oof_preds = np.zeros_like(train_y, dtype=np.float32)\n",
    "    sub_preds = np.zeros((test_X.shape[0], train_y.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    test_dataset = SingleCellDataset(test_X)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=train_params['batch_size'] * 2, shuffle=False)\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_X)):\n",
    "        print(f\"\\n===== Fold {n_fold+1} =====\")\n",
    "        \n",
    "        # 划分数据\n",
    "        X_train, y_train = train_X[train_idx], train_y[train_idx]\n",
    "        X_valid, y_valid = train_X[valid_idx], train_y[valid_idx]\n",
    "        \n",
    "        train_dataset = SingleCellDataset(X_train, y_train)\n",
    "        valid_dataset = SingleCellDataset(X_valid, y_valid)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_params['batch_size'], shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=train_params['batch_size'] * 2, shuffle=False)\n",
    "        \n",
    "        # 初始化模型\n",
    "        model = model_class(**model_params).to(DEVICE)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=train_params['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=6) # 移除了 verbose\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(train_params['epochs']):\n",
    "            model.train()\n",
    "            for features, targets in train_loader:\n",
    "                features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(features)\n",
    "                loss = loss_fn(targets, predictions)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for features, targets in valid_loader:\n",
    "                    features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "                    predictions = model(features)\n",
    "                    loss = loss_fn(targets, predictions)\n",
    "                    val_loss += loss.item() * len(targets)\n",
    "            \n",
    "            val_loss /= len(valid_dataset)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"transformer_model_fold_{n_fold+1}.pth\")\n",
    "                \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{train_params['epochs']}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "        # 加载最佳模型进行预测\n",
    "        model.load_state_dict(torch.load(f\"transformer_model_fold_{n_fold+1}.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        # OOF 预测\n",
    "        val_preds_list = []\n",
    "        with torch.no_grad():\n",
    "            for features, _ in valid_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                predictions = model(features)\n",
    "                val_preds_list.append(predictions.cpu().numpy())\n",
    "        oof_preds[valid_idx] = np.concatenate(val_preds_list)\n",
    "        \n",
    "        # 【修正】: 内存高效的测试集预测\n",
    "        # 不再将所有批次结果存在一个list中，而是逐批次累加\n",
    "        print(\"Predicting on test set (memory-efficiently)...\")\n",
    "        start_idx = 0\n",
    "        with torch.no_grad():\n",
    "            for features in tqdm(test_loader, desc=f\"Test Prediction Fold {n_fold+1}\"):\n",
    "                features = features.to(DEVICE)\n",
    "                # 直接获得numpy格式的预测结果\n",
    "                predictions_np = model(features).cpu().numpy()\n",
    "                \n",
    "                # 获取当前批次的大小\n",
    "                batch_size = predictions_np.shape[0]\n",
    "                end_idx = start_idx + batch_size\n",
    "                \n",
    "                # 直接将缩放后的预测结果累加到 sub_preds 的对应切片上\n",
    "                sub_preds[start_idx:end_idx, :] += predictions_np / folds.get_n_splits()\n",
    "                \n",
    "                # 更新下一个批次的起始索引\n",
    "                start_idx = end_idx\n",
    "\n",
    "        del model, X_train, y_train, X_valid, y_valid\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    cv_score = correlation_score(train_y, oof_preds)\n",
    "    print(f\"\\nOverall CV Pearson Score: {cv_score:.4f}\")\n",
    "    \n",
    "    return oof_preds, sub_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a249f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model 1 (Cosine Similarity Loss) ---\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch 1/3, Val Loss: -0.6428\n",
      "Epoch 2/3, Val Loss: -0.6430\n",
      "Epoch 3/3, Val Loss: -0.6492\n",
      "\n",
      "===== Fold 2 =====\n",
      "Epoch 1/3, Val Loss: -0.6421\n",
      "Epoch 2/3, Val Loss: -0.6423\n",
      "Epoch 3/3, Val Loss: -0.6525\n",
      "\n",
      "===== Fold 3 =====\n",
      "Epoch 1/3, Val Loss: -0.6423\n",
      "Epoch 2/3, Val Loss: -0.6425\n",
      "Epoch 3/3, Val Loss: -0.6510\n",
      "\n",
      "===== Fold 4 =====\n",
      "Epoch 1/3, Val Loss: -0.6422\n",
      "Epoch 2/3, Val Loss: -0.6424\n",
      "Epoch 3/3, Val Loss: -0.6518\n",
      "\n",
      "===== Fold 5 =====\n",
      "Epoch 1/3, Val Loss: -0.6427\n",
      "Epoch 2/3, Val Loss: -0.6429\n",
      "Epoch 3/3, Val Loss: -0.6530\n",
      "\n",
      "Overall CV Pearson Score: 0.6515\n",
      "\n",
      "--- Saving predictions from Model 1 to disk ---\n",
      "Saved 'oof_preds_cos.npy' and 'sub_preds_cos.npy' successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 模型一: 相关性损失模型 ---\n",
    "print(\"\\n--- Training Model 1 (Cosine Similarity Loss) ---\")\n",
    "\n",
    "set_seed(1024)\n",
    "MODEL1_PARAMS = {\n",
    "    'num_features': train_multi_X.shape[1],\n",
    "    'num_targets': train_multi_y.shape[1],\n",
    "    'seq_len': 16,       # 也可以尝试增加到 32\n",
    "    'd_model': 256,      # 增加宽度\n",
    "    'nhead': 16,         # 增加头数\n",
    "    'num_layers': 3,     # 增加深度\n",
    "    'dim_feedforward': 512, # 增加前馈维度\n",
    "    'dropout': 0.15,     # 略微增加dropout以应对更大的模型\n",
    "}\n",
    "\n",
    "TRAIN1_PARAMS = {\n",
    "    'batch_size': 512,\n",
    "    'epochs': 3, \n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds_cos, sub_preds_cos = train_and_evaluate(\n",
    "    TabularTransformer,\n",
    "    train_multi_X,\n",
    "    train_multi_y,\n",
    "    test_multi_X,\n",
    "    folds,\n",
    "    MODEL1_PARAMS,\n",
    "    TRAIN1_PARAMS,\n",
    "    cosine_similarity_loss\n",
    ")\n",
    "\n",
    "OUTPUT_PATH1 = 'oof_preds_cos_multi.npy'\n",
    "OUTPUT_PATH2 = 'sub_preds_cos_multi.npy'\n",
    "print(\"\\n--- Saving predictions from Model 1 to disk ---\")\n",
    "np.save(OUTPUT_PATH1, oof_preds_cos)\n",
    "np.save(OUTPUT_PATH2, sub_preds_cos)\n",
    "print(f\"Saved {OUTPUT_PATH1} and {OUTPUT_PATH2} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model 2 (MSE Loss with Z-scored Targets) ---\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch 1/10, Val Loss: 0.5880\n",
      "Epoch 2/10, Val Loss: 0.5863\n",
      "Epoch 3/10, Val Loss: 0.5725\n",
      "Epoch 4/10, Val Loss: 0.5661\n",
      "Epoch 5/10, Val Loss: 0.5638\n",
      "Epoch 6/10, Val Loss: 0.5618\n",
      "Epoch 7/10, Val Loss: 0.5603\n",
      "Epoch 8/10, Val Loss: 0.5593\n",
      "Epoch 9/10, Val Loss: 0.5582\n",
      "Epoch 10/10, Val Loss: 0.5578\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.88 GiB for an array with shape (55935, 23418) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     19\u001b[0m TRAIN2_PARAMS \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m512\u001b[39m,\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m8e-4\u001b[39m,\n\u001b[0;32m     23\u001b[0m }\n\u001b[0;32m     25\u001b[0m folds \u001b[38;5;241m=\u001b[39m KFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1337\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m oof_preds_mse, sub_preds_mse \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTabularTransformer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_multi_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_multi_y_zscored\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_multi_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfolds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMODEL2_PARAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTRAIN2_PARAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 使用标准的MSE损失\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Saving predictions from Model 1 to disk ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moof_preds_mse.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, oof_preds_mse)\n",
      "Cell \u001b[1;32mIn[6], line 88\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model_class, train_X, train_y, test_X, folds, model_params, train_params, loss_fn)\u001b[0m\n\u001b[0;32m     86\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m model(features)\n\u001b[0;32m     87\u001b[0m         fold_sub_preds\u001b[38;5;241m.\u001b[39mappend(predictions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m---> 88\u001b[0m sub_preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfold_sub_preds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_n_splits\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m model\n\u001b[0;32m     91\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 4.88 GiB for an array with shape (55935, 23418) and data type float32"
     ]
    }
   ],
   "source": [
    "# --- 模型二: MSE 损失模型 (目标Z-score标准化) ---\n",
    "print(\"\\n--- Training Model 2 (MSE Loss with Z-scored Targets) ---\")\n",
    "\n",
    "# 关键步骤：对目标y进行z-score标准化\n",
    "train_multi_y_zscored = zscore(train_multi_y)\n",
    "\n",
    "set_seed(2048)\n",
    "MODEL2_PARAMS = {\n",
    "    'num_features': train_multi_X.shape[1],\n",
    "    'num_targets': 23418,\n",
    "    'seq_len': 20, # 可以尝试不同的序列长度\n",
    "    'd_model': 240, # d_model必须能被nhead整除\n",
    "    'nhead': 8,\n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward': 600,\n",
    "    'dropout': 0.15,\n",
    "}\n",
    "\n",
    "TRAIN2_PARAMS = {\n",
    "    'batch_size': 512,\n",
    "    'epochs': 10,\n",
    "    'lr': 8e-4,\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=1337)\n",
    "\n",
    "oof_preds_mse, sub_preds_mse = train_and_evaluate(\n",
    "    TabularTransformer,\n",
    "    train_multi_X,\n",
    "    train_multi_y_zscored,\n",
    "    test_multi_X,\n",
    "    folds,\n",
    "    MODEL2_PARAMS,\n",
    "    TRAIN2_PARAMS,\n",
    "    nn.MSELoss() # 使用标准的MSE损失\n",
    ")\n",
    "\n",
    "OUTPUT_PATH3 = 'oof_preds_mse_multi.npy'\n",
    "OUTPUT_PATH4 = 'sub_preds_mse_multi.npy'\n",
    "print(\"\\n--- Saving predictions from Model 1 to disk ---\")\n",
    "np.save(OUTPUT_PATH3, oof_preds_mse)\n",
    "np.save(OUTPUT_PATH4, sub_preds_mse)\n",
    "print(f\"Saved {OUTPUT_PATH3} and {OUTPUT_PATH4} successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Ensembling predictions using z-score ---\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'issparse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m PATH_TRAIN_TGT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_multi_targets.h5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     20\u001b[0m adata_train_tgt \u001b[38;5;241m=\u001b[39m ad\u001b[38;5;241m.\u001b[39mread_h5ad(PATH_TRAIN_TGT)\n\u001b[1;32m---> 21\u001b[0m train_multi_y \u001b[38;5;241m=\u001b[39m adata_train_tgt\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mtoarray() \u001b[38;5;28;01mif\u001b[39;00m \u001b[43missparse\u001b[49m(adata_train_tgt\u001b[38;5;241m.\u001b[39mX) \u001b[38;5;28;01melse\u001b[39;00m adata_train_tgt\u001b[38;5;241m.\u001b[39mX\n\u001b[0;32m     22\u001b[0m oof_preds_ensembled \u001b[38;5;241m=\u001b[39m oof_preds_cos_z\u001b[38;5;66;03m# * 0.55 + oof_preds_mse_z * 0.45\u001b[39;00m\n\u001b[0;32m     23\u001b[0m cv_score \u001b[38;5;241m=\u001b[39m correlation_score(train_multi_y, oof_preds_ensembled)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'issparse' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 最终单元: 集成、融合与提交 ---\n",
    "import anndata as ad\n",
    "OUTPUT_PATH1 = 'oof_preds_cos_multi.npy'\n",
    "OUTPUT_PATH2 = 'sub_preds_cos_multi.npy'\n",
    "OUTPUT_PATH3 = 'oof_preds_mse_multi.npy'\n",
    "OUTPUT_PATH4 = 'sub_preds_mse_multi.npy'\n",
    "# --- 1. 使用 Z-Score 进行模型集成 ---\n",
    "print(\"--- Step 1: Ensembling predictions using z-score ---\")\n",
    "oof_preds_cos = np.load(OUTPUT_PATH1)\n",
    "sub_preds_cos = np.load(OUTPUT_PATH2)\n",
    "oof_preds_mse = np.load(OUTPUT_PATH3)\n",
    "sub_preds_mse = np.load(OUTPUT_PATH4)\n",
    "# a) 对两个模型的OOF预测进行z-score标准化，以统一尺度\n",
    "oof_preds_cos_z = zscore(oof_preds_cos)\n",
    "oof_preds_mse_z = zscore(oof_preds_mse)\n",
    "\n",
    "# b) 对两个模型的测试集预测进行z-score标准化\n",
    "sub_preds_cos_z = zscore(sub_preds_cos)\n",
    "sub_preds_mse_z = zscore(sub_preds_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9274f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blended OOF CV Score: 0.6515\n",
      "\n",
      "--- Step 2: Generating final submission file (safe merge method) ---\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import issparse\n",
    "# c) 加权平均\n",
    "PATH_TRAIN_TGT = \"train_multi_targets.h5ad\"\n",
    "adata_train_tgt = ad.read_h5ad(PATH_TRAIN_TGT)\n",
    "train_multi_y = adata_train_tgt.X.toarray() if issparse(adata_train_tgt.X) else adata_train_tgt.X\n",
    "oof_preds_ensembled = oof_preds_cos_z * 0.55 + oof_preds_mse_z * 0.45\n",
    "cv_score = correlation_score(train_multi_y, oof_preds_ensembled)\n",
    "print(f\"Blended OOF CV Score: {cv_score:.4f}\")\n",
    "\n",
    "sub_preds_ensembled = sub_preds_cos_z * 0.55 + sub_preds_mse_z * 0.45\n",
    "\n",
    "# --- 2. 生成最终提交文件 (安全合并版) ---\n",
    "print(\"\\n--- Step 2: Generating final submission file (safe merge method) ---\")\n",
    "\n",
    "# a) 准备预测结果表格\n",
    "protein_ids = pd.read_hdf('train_multi_targets.h5').columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d768b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pred_df:\n",
    "    continue\n",
    "else:#重新加载pred_df\n",
    "    PATH_TEST_INP  = \"test_multi_inputs.h5ad\"\n",
    "    adata_test_inp = ad.read_h5ad(PATH_TEST_INP)\n",
    "    test_df = pd.DataFrame(index=adata_test_inp.obs_names)\n",
    "    pred_df = pd.DataFrame(sub_preds_ensembled, index=test_df.index, columns=protein_ids)\n",
    "\n",
    "# b) 将预测结果转换为“长”格式\n",
    "pred_long = pred_df.reset_index().rename(columns={'index': 'cell_id'}).melt(\n",
    "    id_vars='cell_id', \n",
    "    var_name='gene_id',\n",
    "    value_name='target'\n",
    ")\n",
    "print(f\"Created a long-format prediction table with {len(pred_long)} rows.\")\n",
    "\n",
    "# c) 加载官方的ID转换表\n",
    "evaluation_ids = pd.read_csv('evaluation_ids.csv')\n",
    "\n",
    "# d) 合并预测与官方ID\n",
    "submission_df = evaluation_ids.merge(pred_long, on=['cell_id', 'gene_id'], how='left')\n",
    "\n",
    "# e) 生成最终提交文件\n",
    "final_submission = submission_df[['row_id', 'target']]\n",
    "\n",
    "# 填充可能存在的空值 (主要是Multiome部分)\n",
    "if final_submission['target'].isnull().any():\n",
    "    print(\"Warning: Some rows could not be matched. Filling NaN with 0.\")\n",
    "    final_submission['target'] = final_submission['target'].fillna(0)\n",
    "\n",
    "final_submission.to_csv('submission_multi.csv', index=False)\n",
    "print(\"\\nSubmission file 'submission_multi.csv' created successfully.\")\n",
    "print(final_submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch201-py39-cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
