{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e704a9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading necessary IDs and test predictions ---\n",
      "\n",
      "[ CITE ] Loading IDs ...\n",
      "[ CITE ] Loading predictions (memmap) ...\n",
      "\n",
      "[ MULTI ] Loading IDs ...\n",
      "[ MULTI ] Loading predictions (memmap) ...\n",
      "\n",
      "--- Step 2 (CITE) : Processing test set predictions in chunks ---\n",
      "[CITE] Cells: 48663 | Targets (all): 140 | Targets (evaluated): 140\n",
      "\n",
      "[CITE] --- Processing Chunk 1/4 ---\n",
      "[CITE] Processing cells from index 0 to 12164 ...\n",
      "\n",
      "[CITE] --- Processing Chunk 2/4 ---\n",
      "[CITE] Processing cells from index 12165 to 24329 ...\n",
      "\n",
      "[CITE] --- Processing Chunk 3/4 ---\n",
      "[CITE] Processing cells from index 24330 to 36494 ...\n",
      "\n",
      "[CITE] --- Processing Chunk 4/4 ---\n",
      "[CITE] Processing cells from index 36495 to 48662 ...\n",
      "\n",
      "[CITE] Finished writing to 'submission.csv'.\n",
      "\n",
      "--- Step 2 (MULTI) : Processing test set predictions in chunks ---\n",
      "[MULTI] Cells: 55935 | Targets (all): 23418 | Targets (evaluated): 23418\n",
      "\n",
      "[MULTI] --- Processing Chunk 1/4 ---\n",
      "[MULTI] Processing cells from index 0 to 13982 ...\n",
      "\n",
      "[MULTI] --- Processing Chunk 2/4 ---\n",
      "[MULTI] Processing cells from index 13983 to 27965 ...\n",
      "\n",
      "[MULTI] --- Processing Chunk 3/4 ---\n",
      "[MULTI] Processing cells from index 27966 to 41948 ...\n",
      "\n",
      "[MULTI] --- Processing Chunk 4/4 ---\n",
      "[MULTI] Processing cells from index 41949 to 55934 ...\n",
      "\n",
      "[MULTI] Finished writing to 'submission.csv'.\n",
      "\n",
      "--- Done. Submission file written to 'submission.csv' ---\n"
     ]
    }
   ],
   "source": [
    "# 同时写入cite数据集和multi数据集的预测结果\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "\n",
    "# ----------------------------- Config -----------------------------\n",
    "N_CHUNKS = 4                      # adjust based on RAM\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "PATH_EVAL_IDS = \"evaluation_ids.csv\"\n",
    "\n",
    "# CITE paths\n",
    "PATH_TEST_CITE_INP  = \"test_cite_inputs.h5ad\"\n",
    "PATH_TRAIN_CITE_TGT = \"train_cite_targets.h5ad\"\n",
    "PATH_SUB_PRED_CITE  = \"sub_preds_cite.npy\"   # shape: [n_cite_cells, n_proteins]\n",
    "\n",
    "# MULTI paths\n",
    "PATH_TEST_MULTI_INP  = \"test_multi_inputs.h5ad\"\n",
    "PATH_TRAIN_MULTI_TGT = \"train_multi_targets.h5ad\"\n",
    "PATH_SUB_PRED_MULTI  = \"sub_preds_multi.npy\" # shape: [n_multi_cells, n_genes]\n",
    "\n",
    "# ----------------------- Load global evaluation map ----------------\n",
    "print(\"--- Step 1: Loading necessary IDs and test predictions ---\")\n",
    "try:\n",
    "    evaluation_ids = pd.read_csv(PATH_EVAL_IDS)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"Error: '{PATH_EVAL_IDS}' not found. This file is required.\")\n",
    "\n",
    "# ---------------------------- Load CITE ----------------------------\n",
    "print(\"\\n[ CITE ] Loading IDs ...\")\n",
    "adata_cite_inp = ad.read_h5ad(PATH_TEST_CITE_INP)\n",
    "cite_cell_ids = pd.Index(adata_cite_inp.obs_names.astype(str))\n",
    "del adata_cite_inp; gc.collect()\n",
    "\n",
    "adata_cite_tgt = ad.read_h5ad(PATH_TRAIN_CITE_TGT)\n",
    "cite_target_ids = pd.Index(adata_cite_tgt.var_names.astype(str))  # proteins\n",
    "del adata_cite_tgt; gc.collect()\n",
    "\n",
    "print(\"[ CITE ] Loading predictions (memmap) ...\")\n",
    "sub_preds_cite = np.load(PATH_SUB_PRED_CITE, mmap_mode='r')  # (n_cells, n_targets)\n",
    "\n",
    "# --------------------------- Load MULTI ----------------------------\n",
    "print(\"\\n[ MULTI ] Loading IDs ...\")\n",
    "adata_multi_inp = ad.read_h5ad(PATH_TEST_MULTI_INP)\n",
    "multi_cell_ids = pd.Index(adata_multi_inp.obs_names.astype(str))\n",
    "del adata_multi_inp; gc.collect()\n",
    "\n",
    "adata_multi_tgt = ad.read_h5ad(PATH_TRAIN_MULTI_TGT)\n",
    "multi_target_ids = pd.Index(adata_multi_tgt.var_names.astype(str))  # genes\n",
    "del adata_multi_tgt; gc.collect()\n",
    "\n",
    "print(\"[ MULTI ] Loading predictions (memmap) ...\")\n",
    "sub_preds_multi = np.load(PATH_SUB_PRED_MULTI, mmap_mode='r')  # (n_cells, n_targets)\n",
    "\n",
    "# ---------------------- Quick sanity assertions --------------------\n",
    "assert sub_preds_cite.shape[0]  == len(cite_cell_ids),  \\\n",
    "    f\"CITE cells mismatch: preds {sub_preds_cite.shape[0]} vs ids {len(cite_cell_ids)}\"\n",
    "assert sub_preds_cite.shape[1]  == len(cite_target_ids), \\\n",
    "    f\"CITE targets mismatch: preds {sub_preds_cite.shape[1]} vs ids {len(cite_target_ids)}\"\n",
    "assert sub_preds_multi.shape[0] == len(multi_cell_ids), \\\n",
    "    f\"MULTI cells mismatch: preds {sub_preds_multi.shape[0]} vs ids {len(multi_cell_ids)}\"\n",
    "assert sub_preds_multi.shape[1] == len(multi_target_ids), \\\n",
    "    f\"MULTI targets mismatch: preds {sub_preds_multi.shape[1]} vs ids {len(multi_target_ids)}\"\n",
    "\n",
    "# ---------------------- Helper: chunked writer ---------------------\n",
    "def write_dataset_in_chunks(\n",
    "    name: str,\n",
    "    preds_memmap: np.memmap,\n",
    "    cell_ids: pd.Index,\n",
    "    target_ids: pd.Index,\n",
    "    eval_ids: pd.DataFrame,\n",
    "    submission_path: str,\n",
    "    n_chunks: int = 4,\n",
    "    write_header: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes predictions for one dataset (CITE or MULTI) into submission_path in chunks.\n",
    "    Only rows present in eval_ids (cell_id, gene_id) are written.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Step 2 ({name}) : Processing test set predictions in chunks ---\")\n",
    "\n",
    "    # Restrict evaluation_ids to this dataset's cells and targets to minimize melt size\n",
    "    target_idx = pd.Index(target_ids)\n",
    "    eval_subset = eval_ids[\n",
    "        eval_ids[\"cell_id\"].isin(cell_ids)\n",
    "    ]\n",
    "    eval_targets = pd.Index(eval_subset[\"gene_id\"].unique().astype(str))\n",
    "    # Keep only targets that actually exist in this dataset\n",
    "    eval_targets = eval_targets[eval_targets.isin(target_idx)]\n",
    "\n",
    "    if len(eval_targets) == 0:\n",
    "        print(f\"[{name}] No matching (cell_id, gene_id) in evaluation_ids. Skipping.\")\n",
    "        return\n",
    "\n",
    "    # Boolean mask to slice prediction columns (targets) efficiently\n",
    "    col_mask = target_idx.isin(eval_targets)\n",
    "    selected_targets = target_idx[col_mask]\n",
    "\n",
    "    print(f\"[{name}] Cells: {len(cell_ids)} | Targets (all): {len(target_ids)} | \"\n",
    "          f\"Targets (evaluated): {len(selected_targets)}\")\n",
    "\n",
    "    # Chunking\n",
    "    n = len(cell_ids)\n",
    "    n_chunks = max(1, int(n_chunks))\n",
    "    base = n // n_chunks\n",
    "    edges = [(i * base, (i + 1) * base) for i in range(n_chunks - 1)]\n",
    "    edges.append(((n_chunks - 1) * base, n))  # last chunk takes remainder\n",
    "\n",
    "    for i, (start_idx, end_idx) in enumerate(edges, start=1):\n",
    "        print(f\"\\n[{name}] --- Processing Chunk {i}/{n_chunks} ---\")\n",
    "        print(f\"[{name}] Processing cells from index {start_idx} to {end_idx - 1} ...\")\n",
    "\n",
    "        chunk_cells = cell_ids[start_idx:end_idx]\n",
    "        # Slice predictions: rows for current cells, columns for evaluated targets only\n",
    "        chunk_preds = preds_memmap[start_idx:end_idx][:, col_mask]\n",
    "\n",
    "        # Build long-format only for evaluated targets\n",
    "        chunk_df = pd.DataFrame(chunk_preds, index=chunk_cells, columns=selected_targets)\n",
    "        chunk_long = (\n",
    "            chunk_df\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"cell_id\"})\n",
    "            .melt(id_vars=\"cell_id\", var_name=\"gene_id\", value_name=\"target\")\n",
    "        )\n",
    "\n",
    "        # Further restrict eval to current chunk's cells before merging\n",
    "        eval_subset_chunk = eval_subset[eval_subset[\"cell_id\"].isin(chunk_cells)]\n",
    "\n",
    "        merged = eval_subset_chunk.merge(\n",
    "            chunk_long, on=[\"cell_id\", \"gene_id\"], how=\"inner\"\n",
    "        )[[\"row_id\", \"target\"]]\n",
    "\n",
    "        # Write/append\n",
    "        mode = \"w\" if (write_header and i == 1) else \"a\"\n",
    "        header = bool(write_header and i == 1)\n",
    "        merged.to_csv(submission_path, index=False, mode=mode, header=header)\n",
    "\n",
    "        # Cleanup\n",
    "        del chunk_df, chunk_long, eval_subset_chunk, merged, chunk_preds\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"\\n[{name}] Finished writing to '{submission_path}'.\")\n",
    "\n",
    "# ------------------- Write CITE first, then MULTI ------------------\n",
    "# CITE: header=True (new file)\n",
    "write_dataset_in_chunks(\n",
    "    name=\"CITE\",\n",
    "    preds_memmap=sub_preds_cite,\n",
    "    cell_ids=cite_cell_ids,\n",
    "    target_ids=cite_target_ids,\n",
    "    eval_ids=evaluation_ids,\n",
    "    submission_path=SUBMISSION_PATH,\n",
    "    n_chunks=N_CHUNKS,\n",
    "    write_header=True,\n",
    ")\n",
    "\n",
    "# MULTI: header=False (append)\n",
    "write_dataset_in_chunks(\n",
    "    name=\"MULTI\",\n",
    "    preds_memmap=sub_preds_multi,\n",
    "    cell_ids=multi_cell_ids,\n",
    "    target_ids=multi_target_ids,\n",
    "    eval_ids=evaluation_ids,\n",
    "    submission_path=SUBMISSION_PATH,\n",
    "    n_chunks=N_CHUNKS,\n",
    "    write_header=False,\n",
    ")\n",
    "\n",
    "print(f\"\\n--- Done. Submission file written to '{SUBMISSION_PATH}' ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "059e5fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cite] wrote -> submission_cite.csv\n",
      "[multi] wrote -> submission_multi.csv\n"
     ]
    }
   ],
   "source": [
    "# 仅写入cite或multi数据集的预测结果\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "\n",
    "def write_submission_part(name: str, n_chunks: int = 4):\n",
    "    \"\"\"\n",
    "    name: 'cite' 或 'multi'\n",
    "    读取 sub_preds_{name}.npy，并写入 submission_{name}.csv（首块用 'w' 覆盖，后续 'a' 追加）\n",
    "    \"\"\"\n",
    "    assert name in {\"cite\", \"multi\"}\n",
    "\n",
    "    # 路径映射\n",
    "    PATH_EVAL_IDS = \"evaluation_ids.csv\"\n",
    "    TEST_INP = {\n",
    "        \"cite\": \"test_cite_inputs.h5ad\",\n",
    "        \"multi\": \"test_multi_inputs.h5ad\",\n",
    "    }\n",
    "    TRAIN_TGT = {\n",
    "        \"cite\": \"train_cite_targets.h5ad\",\n",
    "        \"multi\": \"train_multi_targets.h5ad\",\n",
    "    }\n",
    "    SUB_PRED = {\n",
    "        \"cite\": \"sub_preds_cite.npy\",\n",
    "        \"multi\": \"sub_preds_multi.npy\",\n",
    "    }\n",
    "    OUT_PATH = f\"submission_{name}.csv\"\n",
    "\n",
    "    # 读取评估映射\n",
    "    evaluation_ids = pd.read_csv(PATH_EVAL_IDS, dtype={\"cell_id\": str, \"gene_id\": str})\n",
    "\n",
    "    # 读取测试集 cell_id\n",
    "    adata_inp = ad.read_h5ad(TEST_INP[name])\n",
    "    cell_ids = pd.Index(adata_inp.obs_names.astype(str))\n",
    "    del adata_inp; gc.collect()\n",
    "\n",
    "    # 读取目标（列名顺序）\n",
    "    adata_tgt = ad.read_h5ad(TRAIN_TGT[name])\n",
    "    target_ids = pd.Index(adata_tgt.var_names.astype(str))\n",
    "    del adata_tgt; gc.collect()\n",
    "\n",
    "    # 读取预测（memmap）\n",
    "    preds = np.load(SUB_PRED[name], mmap_mode=\"r\")\n",
    "    # 基本对齐检查（可按需注释）\n",
    "    assert preds.shape[0] == len(cell_ids), f\"{name}: cell数不匹配\"\n",
    "    assert preds.shape[1] == len(target_ids), f\"{name}: target数不匹配\"\n",
    "\n",
    "    # 只保留需要评估的 (cell_id, gene_id)，并仅 melt 被评估到的 targets，减少内存\n",
    "    eval_subset = evaluation_ids[evaluation_ids[\"cell_id\"].isin(cell_ids)]\n",
    "    eval_targets = pd.Index(eval_subset[\"gene_id\"].unique())\n",
    "    col_mask = target_ids.isin(eval_targets)\n",
    "    selected_targets = target_ids[col_mask]\n",
    "\n",
    "    # 分块\n",
    "    n = len(cell_ids)\n",
    "    n_chunks = max(1, int(n_chunks))\n",
    "    base = n // n_chunks\n",
    "    edges = [(i * base, (i + 1) * base) for i in range(n_chunks - 1)]\n",
    "    edges.append(((n_chunks - 1) * base, n))\n",
    "\n",
    "    for i, (s, e) in enumerate(edges, start=1):\n",
    "        chunk_cells = cell_ids[s:e]\n",
    "        chunk_preds = preds[s:e][:, col_mask]\n",
    "\n",
    "        df = pd.DataFrame(chunk_preds, index=chunk_cells, columns=selected_targets)\n",
    "        long_df = (\n",
    "            df.reset_index()\n",
    "              .rename(columns={\"index\": \"cell_id\"})\n",
    "              .melt(id_vars=\"cell_id\", var_name=\"gene_id\", value_name=\"target\")\n",
    "        )\n",
    "        eval_chunk = eval_subset[eval_subset[\"cell_id\"].isin(chunk_cells)]\n",
    "        merged = eval_chunk.merge(long_df, on=[\"cell_id\", \"gene_id\"], how=\"inner\")[[\"row_id\", \"target\"]]\n",
    "\n",
    "        mode = \"w\" if i == 1 else \"a\"     # 首块覆盖写，其余块追加\n",
    "        header = (i == 1)\n",
    "        merged.to_csv(OUT_PATH, index=False, mode=mode, header=header)\n",
    "\n",
    "        del df, long_df, eval_chunk, merged, chunk_preds\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"[{name}] wrote -> {OUT_PATH}\")\n",
    "\n",
    "write_submission_part('cite')\n",
    "write_submission_part('multi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0049002e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged -> submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 合并cite和multi数据集的预测结果\n",
    "def merge_submissions():\n",
    "    \"\"\"\n",
    "    无检查地合并 'submission_cite.csv' 与 'submission_multi.csv' 到 'submission.csv'\n",
    "    \"\"\"\n",
    "    df_cite  = pd.read_csv(\"submission_cite.csv\")\n",
    "    df_multi = pd.read_csv(\"submission_multi.csv\")\n",
    "    df = pd.concat([df_cite, df_multi], ignore_index=True)\n",
    "    df.to_csv(\"submission.csv\", index=False)\n",
    "    print(\"merged -> submission.csv\")\n",
    "\n",
    "merge_submissions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch201-py39-cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
