{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# 核心库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 进度条\n",
    "from tqdm.notebook import tqdm\n",
    "name = 'cite'\n",
    "# --- 全局配置 ---\n",
    "# 设备配置，优先使用GPU\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# 为了结果可复现，设置随机种子\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10f9a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed .h5ad files...\n",
      "Extracting data into NumPy arrays for training...\n",
      "\n",
      "Data shapes for model training:\n",
      "train_cite_X: (64074, 15435)\n",
      "train_cite_y: (64074, 140)\n",
      "test_cite_X: (48663, 15435) (placeholder data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# --- 数据路径 ---\n",
    "# 这是我们之前数据清洗步骤生成的文件的路径\n",
    "PATH_TRAIN_INP = \"train_cite_inputs.h5ad\"\n",
    "PATH_TRAIN_TGT = \"train_cite_targets.h5ad\"\n",
    "PATH_TEST_INP  = \"test_cite_inputs.h5ad\" # 假设测试集也已处理\n",
    "PATH_META      = \"metadata.csv\"\n",
    "\n",
    "# 加载已清洗的训练数据\n",
    "print(\"Loading pre-processed .h5ad files...\")\n",
    "adata_train_inp = ad.read_h5ad(PATH_TRAIN_INP)\n",
    "adata_train_tgt = ad.read_h5ad(PATH_TRAIN_TGT)\n",
    "adata_test_inp = ad.read_h5ad(PATH_TEST_INP) # 如果需要加载测试集\n",
    "\n",
    "# --- 关键步骤：数据对齐 ---\n",
    "# 确保输入和目标的细胞顺序完全一致\n",
    "assert np.all(adata_train_inp.obs_names == adata_train_tgt.obs_names), \\\n",
    "    \"Error: Train inputs and targets have different cell orders!\"\n",
    "\n",
    "# 加载元数据并对齐到训练数据的顺序\n",
    "meta_df = pd.read_csv(PATH_META).set_index(\"cell_id\")\n",
    "meta_df = meta_df.loc[adata_train_inp.obs_names]\n",
    "\n",
    "# --- 准备最终的模型输入变量 ---\n",
    "# 将特征矩阵和目标矩阵提取为NumPy数组\n",
    "# 如果矩阵是稀疏的(issparse)，则转换为稠密的.toarray()\n",
    "print(\"Extracting data into NumPy arrays for training...\")\n",
    "train_cite_X = adata_train_inp.X.toarray() if issparse(adata_train_inp.X) else adata_train_inp.X\n",
    "train_cite_y = adata_train_tgt.X.toarray() if issparse(adata_train_tgt.X) else adata_train_tgt.X\n",
    "\n",
    "#加载测试集数据\n",
    "test_cite_X = adata_test_inp.X\n",
    "test_df = pd.DataFrame(index=adata_test_inp.obs_names)\n",
    "\n",
    "# 打印数据维度以确认\n",
    "print(\"\\nData shapes for model training:\")\n",
    "print(f\"train_cite_X: {train_cite_X.shape}\")\n",
    "print(f\"train_cite_y: {train_cite_y.shape}\")\n",
    "print(f\"test_cite_X: {test_cite_X.shape} (placeholder data)\")\n",
    "\n",
    "# 清理内存\n",
    "del adata_train_inp, adata_train_tgt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e86af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 评估与数据处理工具 ---\n",
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"逐行计算皮尔逊相关系数并返回平均值\"\"\"\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)\n",
    "\n",
    "def zscore(x):\n",
    "    \"\"\"对输入矩阵的每一行进行Z-score标准化\"\"\"\n",
    "    x_zscore = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_row = x[i]\n",
    "        mean = np.mean(x_row)\n",
    "        std = np.std(x_row)\n",
    "        if std == 0: # 防止除以零\n",
    "            x_zscore.append(x_row - mean)\n",
    "        else:\n",
    "            x_zscore.append((x_row - mean) / std)\n",
    "    return np.array(x_zscore)\n",
    "\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    \"\"\"皮尔逊相关性损失的PyTorch实现\"\"\"\n",
    "    # 1. 中心化\n",
    "    y_true_centered = y_true - torch.mean(y_true, dim=1, keepdim=True)\n",
    "    y_pred_centered = y_pred - torch.mean(y_pred, dim=1, keepdim=True)\n",
    "    \n",
    "    # 2. L2标准化\n",
    "    y_true_norm = torch.nn.functional.normalize(y_true_centered, p=2, dim=1)\n",
    "    y_pred_norm = torch.nn.functional.normalize(y_pred_centered, p=2, dim=1)\n",
    "    \n",
    "    # 3. 计算余弦相似度并取反作为损失\n",
    "    # .mean()聚合batch中的所有样本损失\n",
    "    return -torch.nn.CosineSimilarity(dim=1)(y_true_norm, y_pred_norm).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136cee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse # Make sure issparse is imported\n",
    "\n",
    "class SingleCellDataset(Dataset):\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.is_train = targets is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        # 【修正】: 使用 .shape[0] 来获取稀疏矩阵的行数（样本数）\n",
    "        # [FIX]: Use .shape[0] to get the number of rows (samples) from the sparse matrix\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This part, which handles slicing, is correct from our previous fix.\n",
    "        feature = self.features[idx]\n",
    "        if issparse(feature):\n",
    "            feature = feature.toarray().squeeze()\n",
    "        \n",
    "        feature = torch.tensor(feature, dtype=torch.float32)\n",
    "        \n",
    "        if self.is_train:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return feature, target\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baec4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, seq_len=16, d_model=256, nhead=8, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        \"\"\"\n",
    "        num_features: 原始输入特征数 (例如: 20000+)\n",
    "        num_targets: 目标输出数 (140)\n",
    "        seq_len: 我们要将原始特征向量“重塑”成的序列长度\n",
    "        d_model: Transformer内部的工作维度 (必须能被nhead整除)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. 线性投射层：将原始的高维特征投射到一个可以被重塑为序列的空间\n",
    "        self.projector = nn.Linear(num_features, seq_len * d_model)\n",
    "        \n",
    "        # 2. CLS Token: 类似于BERT，我们添加一个特殊的可学习的token，用于聚合整个序列的信息\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        \n",
    "        # 3. 位置编码：让模型知道序列中每个元素的位置信息\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, seq_len + 1, d_model))\n",
    "        \n",
    "        # 4. 标准的Transformer编码器\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 5. 预测头：一个简单的MLP，接收CLS Token的最终输出，并预测140个蛋白质\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_targets)\n",
    "        )\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_features)\n",
    "        \n",
    "        # 1. 投射并重塑为序列\n",
    "        x = self.projector(x) # -> (batch_size, seq_len * d_model)\n",
    "        x = x.reshape(-1, self.seq_len, self.d_model) # -> (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 2. 添加CLS token\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # -> (batch_size, 1, d_model)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # -> (batch_size, seq_len + 1, d_model)\n",
    "        \n",
    "        # 3. 添加位置编码\n",
    "        x += self.pos_encoder\n",
    "        \n",
    "        # 4. 通过Transformer编码器\n",
    "        x = self.transformer_encoder(x) # -> (batch_size, seq_len + 1, d_model)\n",
    "        \n",
    "        # 5. 取出CLS token的输出，并通过预测头\n",
    "        cls_output = x[:, 0] # -> (batch_size, d_model)\n",
    "        output = self.mlp_head(cls_output) # -> (batch_size, num_targets)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a31562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model_class,\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    folds,\n",
    "    model_params,\n",
    "    train_params,\n",
    "    loss_fn\n",
    "):\n",
    "    oof_preds = np.zeros_like(train_y, dtype=np.float32)\n",
    "    sub_preds = np.zeros((test_X.shape[0], train_y.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    test_dataset = SingleCellDataset(test_X)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=train_params['batch_size'] * 2, shuffle=False)\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_X)):\n",
    "        print(f\"\\n===== Fold {n_fold+1} =====\")\n",
    "        \n",
    "        # 划分数据\n",
    "        X_train, y_train = train_X[train_idx], train_y[train_idx]\n",
    "        X_valid, y_valid = train_X[valid_idx], train_y[valid_idx]\n",
    "        \n",
    "        train_dataset = SingleCellDataset(X_train, y_train)\n",
    "        valid_dataset = SingleCellDataset(X_valid, y_valid)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_params['batch_size'], shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=train_params['batch_size'] * 2, shuffle=False)\n",
    "        \n",
    "        # 初始化模型\n",
    "        model = model_class(**model_params).to(DEVICE)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=train_params['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=6, verbose=True)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(train_params['epochs']):\n",
    "            model.train()\n",
    "            for features, targets in train_loader:\n",
    "                features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(features)\n",
    "                loss = loss_fn(targets, predictions)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_preds = []\n",
    "            with torch.no_grad():\n",
    "                for features, targets in valid_loader:\n",
    "                    features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "                    predictions = model(features)\n",
    "                    loss = loss_fn(targets, predictions)\n",
    "                    val_loss += loss.item() * len(targets)\n",
    "                    val_preds.append(predictions.cpu().numpy())\n",
    "            \n",
    "            val_loss /= len(valid_dataset)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"transformer_{name}_fold_{n_fold+1}.pth\")\n",
    "                \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{train_params['epochs']}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "        # 加载最佳模型进行预测\n",
    "        model.load_state_dict(torch.load(f\"transformer_{cite}_fold_{n_fold+1}.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        # OOF 预测\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for features, _ in valid_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                predictions = model(features)\n",
    "                val_preds.append(predictions.cpu().numpy())\n",
    "        oof_preds[valid_idx] = np.concatenate(val_preds)\n",
    "        \n",
    "        # 测试集预测\n",
    "        fold_sub_preds = []\n",
    "        with torch.no_grad():\n",
    "            for features in test_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                predictions = model(features)\n",
    "                fold_sub_preds.append(predictions.cpu().numpy())\n",
    "        sub_preds += np.concatenate(fold_sub_preds) / folds.get_n_splits()\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    cv_score = correlation_score(train_y, oof_preds)\n",
    "    print(f\"\\nOverall CV Pearson Score: {cv_score:.4f}\")\n",
    "    \n",
    "    return oof_preds, sub_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67a249f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model 1 (Cosine Similarity Loss) ---\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch 10/30, Val Loss: -0.8092\n",
      "Epoch 20/30, Val Loss: -0.8619\n",
      "Epoch 30/30, Val Loss: -0.8882\n",
      "\n",
      "===== Fold 2 =====\n",
      "Epoch 10/30, Val Loss: -0.8088\n",
      "Epoch 20/30, Val Loss: -0.8088\n",
      "Epoch 30/30, Val Loss: -0.8088\n",
      "\n",
      "===== Fold 3 =====\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 10/30, Val Loss: -0.8092\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 20/30, Val Loss: -0.8092\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 30/30, Val Loss: -0.8092\n",
      "\n",
      "===== Fold 4 =====\n",
      "Epoch 10/30, Val Loss: -0.8080\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 20/30, Val Loss: -0.8080\n",
      "Epoch 30/30, Val Loss: -0.8080\n",
      "\n",
      "===== Fold 5 =====\n",
      "Epoch 10/30, Val Loss: -0.8837\n",
      "Epoch 20/30, Val Loss: -0.8945\n",
      "Epoch 30/30, Val Loss: -0.8005\n",
      "\n",
      "Overall CV Pearson Score: 0.8430\n",
      "\n",
      "--- Saving predictions from Model 1 to disk ---\n",
      "Saved 'oof_preds_cos_cite.npy' and 'sub_preds_cos_cite.npy' successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 模型一: 相关性损失模型 ---\n",
    "print(\"\\n--- Training Model 1 (Cosine Similarity Loss) ---\")\n",
    "\n",
    "set_seed(1024)\n",
    "MODEL1_PARAMS = {\n",
    "    'num_features': train_cite_X.shape[1],\n",
    "    'num_targets': 140,\n",
    "    'seq_len': 16,\n",
    "    'd_model': 256,\n",
    "    'nhead': 16,\n",
    "    'num_layers': 3,\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "TRAIN1_PARAMS = {\n",
    "    'batch_size': 512,\n",
    "    'epochs': 30, \n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds_cos, sub_preds_cos = train_and_evaluate(\n",
    "    TabularTransformer,\n",
    "    train_cite_X,\n",
    "    train_cite_y,\n",
    "    test_cite_X,\n",
    "    folds,\n",
    "    MODEL1_PARAMS,\n",
    "    TRAIN1_PARAMS,\n",
    "    cosine_similarity_loss\n",
    ")\n",
    "print(\"\\n--- Saving predictions from Model 1 to disk ---\")\n",
    "np.save('oof_preds_cos_cite.npy', oof_preds_cos)\n",
    "np.save('sub_preds_cos_cite.npy', sub_preds_cos)\n",
    "print(\"Saved 'oof_preds_cos_cite.npy' and 'sub_preds_cos_cite.npy' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9622b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model 2 (MSE Loss with Z-scored Targets) ---\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch 10/30, Val Loss: 0.3461\n",
      "Epoch 00017: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 20/30, Val Loss: 0.3460\n",
      "Epoch 00025: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 30/30, Val Loss: 0.3460\n",
      "\n",
      "===== Fold 2 =====\n",
      "Epoch 10/30, Val Loss: 0.3470\n",
      "Epoch 00016: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 20/30, Val Loss: 0.3468\n",
      "Epoch 00024: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 30/30, Val Loss: 0.3467\n",
      "\n",
      "===== Fold 3 =====\n",
      "Epoch 10/30, Val Loss: 0.3475\n",
      "Epoch 00020: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 20/30, Val Loss: 0.3476\n",
      "Epoch 00027: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 30/30, Val Loss: 0.3474\n",
      "\n",
      "===== Fold 4 =====\n",
      "Epoch 00010: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 10/30, Val Loss: 0.3433\n",
      "Epoch 00017: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 20/30, Val Loss: 0.3431\n",
      "Epoch 00024: reducing learning rate of group 0 to 8.0000e-07.\n",
      "Epoch 30/30, Val Loss: 0.3431\n",
      "\n",
      "===== Fold 5 =====\n",
      "Epoch 10/30, Val Loss: 0.3457\n",
      "Epoch 00020: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 20/30, Val Loss: 0.3456\n",
      "Epoch 00028: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 30/30, Val Loss: 0.3455\n",
      "\n",
      "Overall CV Pearson Score: 0.8089\n",
      "\n",
      "--- Saving predictions from Model 1 to disk ---\n",
      "Saved 'oof_preds_cos_cite.npy' and 'sub_preds_cos_cite.npy' successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 模型二: MSE 损失模型 (目标Z-score标准化) ---\n",
    "print(\"\\n--- Training Model 2 (MSE Loss with Z-scored Targets) ---\")\n",
    "\n",
    "# 关键步骤：对目标y进行z-score标准化\n",
    "train_cite_y_zscored = zscore(train_cite_y)\n",
    "\n",
    "set_seed(2048)\n",
    "MODEL2_PARAMS = {\n",
    "    'num_features': train_cite_X.shape[1],\n",
    "    'num_targets': 140,\n",
    "    'seq_len': 20, # 可以尝试不同的序列长度\n",
    "    'd_model': 240, # d_model必须能被nhead整除\n",
    "    'nhead': 16,\n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward': 600,\n",
    "    'dropout': 0.15,\n",
    "}\n",
    "\n",
    "TRAIN2_PARAMS = {\n",
    "    'batch_size': 512,\n",
    "    'epochs': 30,\n",
    "    'lr': 8e-4,\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=1337)\n",
    "\n",
    "oof_preds_mse, sub_preds_mse = train_and_evaluate(\n",
    "    TabularTransformer,\n",
    "    train_cite_X,\n",
    "    train_cite_y_zscored,\n",
    "    test_cite_X,\n",
    "    folds,\n",
    "    MODEL2_PARAMS,\n",
    "    TRAIN2_PARAMS,\n",
    "    nn.MSELoss() # 使用标准的MSE损失\n",
    ")\n",
    "print(\"\\n--- Saving predictions from Model 1 to disk ---\")\n",
    "np.save('oof_preds_cos_cite.npy', oof_preds_cos)\n",
    "np.save('sub_preds_cos_cite.npy', sub_preds_cos)\n",
    "print(\"Saved 'oof_preds_cos_cite.npy' and 'sub_preds_cos_cite.npy' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd2343a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Ensembling predictions using z-score ---\n",
      "Blended OOF CV Score: 0.8365\n",
      "\n",
      "--- Step 2: Generating final submission file (safe merge method) ---\n",
      "Created a long-format prediction table with 6812820 rows.\n",
      "Warning: Some rows could not be matched. Filling NaN with 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\temp\\ipykernel_4448\\2216641364.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_submission['target'] = final_submission['target'].fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission file 'submission_cite.csv' created successfully.\n",
      "   row_id    target\n",
      "0       0 -0.543250\n",
      "1       1 -0.535083\n",
      "2       2 -0.370595\n",
      "3       3  1.383205\n",
      "4       4  1.342296\n"
     ]
    }
   ],
   "source": [
    "# --- 最终单元: 集成、融合与提交 ---\n",
    "\n",
    "\n",
    "# --- 1. 使用 Z-Score 进行模型集成 ---\n",
    "print(\"--- Step 1: Ensembling predictions using z-score ---\")\n",
    "\n",
    "# a) 对两个模型的OOF预测进行z-score标准化，以统一尺度\n",
    "oof_preds_cos_z = zscore(oof_preds_cos)\n",
    "oof_preds_mse_z = zscore(oof_preds_mse) # 同样进行z-score是更严谨的做法\n",
    "\n",
    "# b) 对两个模型的测试集预测进行z-score标准化\n",
    "sub_preds_cos_z = zscore(sub_preds_cos)\n",
    "sub_preds_mse_z = zscore(sub_preds_mse)\n",
    "\n",
    "# c) 加权平均\n",
    "oof_preds_ensembled = oof_preds_cos_z * 0.55 + oof_preds_mse_z * 0.45\n",
    "cv_score = correlation_score(train_cite_y, oof_preds_ensembled)\n",
    "print(f\"Blended OOF CV Score: {cv_score:.4f}\")\n",
    "\n",
    "sub_preds_ensembled = sub_preds_cos_z * 0.55 + sub_preds_mse_z * 0.45\n",
    "\n",
    "# --- 2. 生成最终提交文件 (安全合并版) ---\n",
    "print(\"\\n--- Step 2: Generating final submission file (safe merge method) ---\")\n",
    "\n",
    "# a) 准备预测结果表格\n",
    "import anndata as ad\n",
    "adata_targets = ad.read_h5ad('train_cite_targets.h5ad')\n",
    "protein_ids = adata_targets.var_names\n",
    "cite_pred_df = pd.DataFrame(sub_preds_ensembled, index=test_df.index, columns=protein_ids)\n",
    "\n",
    "# b) 将预测结果转换为“长”格式\n",
    "cite_pred_long = cite_pred_df.reset_index().rename(columns={'index': 'cell_id'}).melt(\n",
    "    id_vars='cell_id', \n",
    "    var_name='gene_id',\n",
    "    value_name='target'\n",
    ")\n",
    "print(f\"Created a long-format prediction table with {len(cite_pred_long)} rows.\")\n",
    "\n",
    "# c) 加载官方的ID转换表\n",
    "evaluation_ids = pd.read_csv('evaluation_ids.csv')\n",
    "\n",
    "# d) 合并预测与官方ID\n",
    "submission_df = evaluation_ids.merge(cite_pred_long, on=['cell_id', 'gene_id'], how='left')\n",
    "\n",
    "# e) 生成最终提交文件\n",
    "final_submission = submission_df[['row_id', 'target']]\n",
    "\n",
    "# 填充可能存在的空值 (主要是Multiome部分)\n",
    "if final_submission['target'].isnull().any():\n",
    "    print(\"Warning: Some rows could not be matched. Filling NaN with 0.\")\n",
    "    final_submission['target'] = final_submission['target'].fillna(0)\n",
    "\n",
    "final_submission.to_csv('submission_cite.csv', index=False)\n",
    "print(\"\\nSubmission file 'submission_cite.csv' created successfully.\")\n",
    "print(final_submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch201-py39-cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
