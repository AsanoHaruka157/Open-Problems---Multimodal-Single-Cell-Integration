{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f85063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "name = 'cite'\n",
    "# --- Global Configuration ---\n",
    "# Device configuration, use GPU if available\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f9a199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed .h5ad files...\n",
      "Extracting data into NumPy arrays for training...\n",
      "\n",
      "Data shapes for model training:\n",
      "train_cite_X: (64074, 15435)\n",
      "train_cite_y: (64074, 140)\n",
      "test_cite_X: (48663, 15435) (placeholder data)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# --- Data Paths ---\n",
    "# Path to the files generated from our previous data cleaning steps\n",
    "PATH_TRAIN_INP = \"train_cite_inputs.h5ad\"\n",
    "PATH_TRAIN_TGT = \"train_cite_targets.h5ad\"\n",
    "PATH_TEST_INP  = \"test_cite_inputs.h5ad\"\n",
    "PATH_META      = \"metadata.csv\"\n",
    "\n",
    "# Load the cleaned training data\n",
    "print(\"Loading pre-processed .h5ad files...\")\n",
    "adata_train_inp = ad.read_h5ad(PATH_TRAIN_INP)\n",
    "adata_train_tgt = ad.read_h5ad(PATH_TRAIN_TGT)\n",
    "adata_test_inp = ad.read_h5ad(PATH_TEST_INP)\n",
    "\n",
    "# Ensure the cell order of inputs and targets is exactly the same\n",
    "assert np.all(adata_train_inp.obs_names == adata_train_tgt.obs_names), \\\n",
    "    \"Error: Train inputs and targets have different cell orders!\"\n",
    "\n",
    "# Load metadata and align it to the order of the training data\n",
    "meta_df = pd.read_csv(PATH_META).set_index(\"cell_id\")\n",
    "meta_df = meta_df.loc[adata_train_inp.obs_names]\n",
    "\n",
    "# --- Prepare final model input variables ---\n",
    "# Extract feature and target matrices as NumPy arrays\n",
    "# If the matrix is sparse (issparse), convert it to dense using .toarray()\n",
    "print(\"Extracting data into NumPy arrays for training...\")\n",
    "train_cite_X = adata_train_inp.X.toarray() if issparse(adata_train_inp.X) else adata_train_inp.X\n",
    "train_cite_y = adata_train_tgt.X.toarray() if issparse(adata_train_tgt.X) else adata_train_tgt.X\n",
    "\n",
    "# Load test set data\n",
    "test_cite_X = adata_test_inp.X\n",
    "test_df = pd.DataFrame(index=adata_test_inp.obs_names)\n",
    "\n",
    "# Print data dimensions to confirm\n",
    "print(\"\\nData shapes for model training:\")\n",
    "print(f\"train_cite_X: {train_cite_X.shape}\")\n",
    "print(f\"train_cite_y: {train_cite_y.shape}\")\n",
    "print(f\"test_cite_X: {test_cite_X.shape} (placeholder data)\")\n",
    "\n",
    "# Clean up memory\n",
    "del adata_train_inp, adata_train_tgt\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e86af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation and Data Processing Tools ---\n",
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    \"\"\"Calculate Pearson correlation coefficient row by row and return the average\"\"\"\n",
    "    corrsum = 0\n",
    "    for i in range(len(y_true)):\n",
    "        corrsum += np.corrcoef(y_true[i], y_pred[i])[1, 0]\n",
    "    return corrsum / len(y_true)\n",
    "\n",
    "def zscore(x):\n",
    "    \"\"\"Perform Z-score normalization on each row of the input matrix\"\"\"\n",
    "    x_zscore = []\n",
    "    for i in range(x.shape[0]):\n",
    "        x_row = x[i]\n",
    "        mean = np.mean(x_row)\n",
    "        std = np.std(x_row)\n",
    "        if std == 0: # Prevent division by zero\n",
    "            x_zscore.append(x_row - mean)\n",
    "        else:\n",
    "            x_zscore.append((x_row - mean) / std)\n",
    "    return np.array(x_zscore)\n",
    "\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    \"\"\"PyTorch implementation of Pearson correlation loss\"\"\"\n",
    "    # 1. Centering\n",
    "    y_true_centered = y_true - torch.mean(y_true, dim=1, keepdim=True)\n",
    "    y_pred_centered = y_pred - torch.mean(y_pred, dim=1, keepdim=True)\n",
    "    \n",
    "    # 2. L2 Normalization\n",
    "    y_true_norm = torch.nn.functional.normalize(y_true_centered, p=2, dim=1)\n",
    "    y_pred_norm = torch.nn.functional.normalize(y_pred_centered, p=2, dim=1)\n",
    "    \n",
    "    # 3. Calculate cosine similarity and negate it as loss\n",
    "    # .mean() aggregates all sample losses in the batch\n",
    "    return -torch.nn.CosineSimilarity(dim=1)(y_true_norm, y_pred_norm).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136cee86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import issparse # Make sure issparse is imported\n",
    "\n",
    "class SingleCellDataset(Dataset):\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.is_train = targets is not None\n",
    "\n",
    "    def __len__(self):\n",
    "        # Use .shape[0] to get the number of rows (samples) from the sparse matrix\n",
    "        return self.features.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This part, which handles slicing, is correct from our previous fix.\n",
    "        feature = self.features[idx]\n",
    "        if issparse(feature):\n",
    "            feature = feature.toarray().squeeze()\n",
    "        \n",
    "        feature = torch.tensor(feature, dtype=torch.float32)\n",
    "        \n",
    "        if self.is_train:\n",
    "            target = torch.tensor(self.targets[idx], dtype=torch.float32)\n",
    "            return feature, target\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec4258",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTransformer(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, seq_len=16, d_model=256, nhead=8, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        \"\"\"\n",
    "        num_features: Number of original input features (e.g., 20000+)\n",
    "        num_targets: Number of target outputs (140)\n",
    "        seq_len: The sequence length to \"reshape\" the original feature vector into\n",
    "        d_model: The working dimension inside the Transformer (must be divisible by nhead)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Linear Projection Layer: Projects the original high-dimensional features into a space that can be reshaped into a sequence\n",
    "        self.projector = nn.Linear(num_features, seq_len * d_model)\n",
    "        \n",
    "        # 2. CLS Token: Similar to BERT, we add a special learnable token to aggregate information from the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        \n",
    "        # 3. Positional Encoding: Allows the model to know the position information of each element in the sequence\n",
    "        self.pos_encoder = nn.Parameter(torch.randn(1, seq_len + 1, d_model))\n",
    "        \n",
    "        # 4. Standard Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 5. Prediction Head: A simple MLP that receives the final output of the CLS Token and predicts 140 proteins\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, num_targets)\n",
    "        )\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, num_features)\n",
    "        \n",
    "        # 1. Project and reshape into a sequence\n",
    "        x = self.projector(x) # -> (batch_size, seq_len * d_model)\n",
    "        x = x.reshape(-1, self.seq_len, self.d_model) # -> (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # 2. Add CLS token\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # -> (batch_size, 1, d_model)\n",
    "        x = torch.cat((cls_tokens, x), dim=1) # -> (batch_size, seq_len + 1, d_model)\n",
    "        \n",
    "        # 3. Add positional encoding\n",
    "        x += self.pos_encoder\n",
    "        \n",
    "        # 4. Transformer encoder\n",
    "        x = self.transformer_encoder(x) # -> (batch_size, seq_len + 1, d_model)\n",
    "        \n",
    "        # 5. Take the output of the CLS token and pass it through the prediction head\n",
    "        cls_output = x[:, 0] # -> (batch_size, d_model)\n",
    "        output = self.mlp_head(cls_output) # -> (batch_size, num_targets)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a31562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    model_class,\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    folds,\n",
    "    model_params,\n",
    "    train_params,\n",
    "    loss_fn\n",
    "):\n",
    "    oof_preds = np.zeros_like(train_y, dtype=np.float32)\n",
    "    sub_preds = np.zeros((test_X.shape[0], train_y.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    test_dataset = SingleCellDataset(test_X)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=train_params['batch_size'] * 2, shuffle=False)\n",
    "\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_X)):\n",
    "        print(f\"\\n===== Fold {n_fold+1} =====\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, y_train = train_X[train_idx], train_y[train_idx]\n",
    "        X_valid, y_valid = train_X[valid_idx], train_y[valid_idx]\n",
    "        \n",
    "        train_dataset = SingleCellDataset(X_train, y_train)\n",
    "        valid_dataset = SingleCellDataset(X_valid, y_valid)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=train_params['batch_size'], shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=train_params['batch_size'] * 2, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = model_class(**model_params).to(DEVICE)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=train_params['lr'])\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=6, verbose=True)\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        \n",
    "        for epoch in range(train_params['epochs']):\n",
    "            model.train()\n",
    "            for features, targets in train_loader:\n",
    "                features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                predictions = model(features)\n",
    "                loss = loss_fn(targets, predictions)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            val_preds = []\n",
    "            with torch.no_grad():\n",
    "                for features, targets in valid_loader:\n",
    "                    features, targets = features.to(DEVICE), targets.to(DEVICE)\n",
    "                    predictions = model(features)\n",
    "                    loss = loss_fn(targets, predictions)\n",
    "                    val_loss += loss.item() * len(targets)\n",
    "                    val_preds.append(predictions.cpu().numpy())\n",
    "            \n",
    "            val_loss /= len(valid_dataset)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"transformer_{name}_fold_{n_fold+1}.pth\")\n",
    "                \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{train_params['epochs']}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "        # Load best model for prediction\n",
    "        model.load_state_dict(torch.load(f\"transformer_{cite}_fold_{n_fold+1}.pth\"))\n",
    "        model.eval()\n",
    "        \n",
    "        # OOF predictions\n",
    "        val_preds = []\n",
    "        with torch.no_grad():\n",
    "            for features, _ in valid_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                predictions = model(features)\n",
    "                val_preds.append(predictions.cpu().numpy())\n",
    "        oof_preds[valid_idx] = np.concatenate(val_preds)\n",
    "        \n",
    "        # Test set predictions\n",
    "        fold_sub_preds = []\n",
    "        with torch.no_grad():\n",
    "            for features in test_loader:\n",
    "                features = features.to(DEVICE)\n",
    "                predictions = model(features)\n",
    "                fold_sub_preds.append(predictions.cpu().numpy())\n",
    "        sub_preds += np.concatenate(fold_sub_preds) / folds.get_n_splits()\n",
    "\n",
    "        del model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    cv_score = correlation_score(train_y, oof_preds)\n",
    "    print(f\"\\nOverall CV Pearson Score: {cv_score:.4f}\")\n",
    "    \n",
    "    return oof_preds, sub_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a249f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model 1 (Cosine Similarity Loss) ---\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch 10/30, Val Loss: -0.8092\n",
      "Epoch 20/30, Val Loss: -0.8619\n",
      "Epoch 30/30, Val Loss: -0.8882\n",
      "\n",
      "===== Fold 2 =====\n",
      "Epoch 10/30, Val Loss: -0.8088\n",
      "Epoch 20/30, Val Loss: -0.8088\n",
      "Epoch 30/30, Val Loss: -0.8088\n",
      "\n",
      "===== Fold 3 =====\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 10/30, Val Loss: -0.8092\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 20/30, Val Loss: -0.8092\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00030: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 30/30, Val Loss: -0.8092\n",
      "\n",
      "===== Fold 4 =====\n",
      "Epoch 10/30, Val Loss: -0.8080\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 20/30, Val Loss: -0.8080\n",
      "Epoch 30/30, Val Loss: -0.8080\n",
      "\n",
      "===== Fold 5 =====\n",
      "Epoch 10/30, Val Loss: -0.8837\n",
      "Epoch 20/30, Val Loss: -0.8945\n",
      "Epoch 30/30, Val Loss: -0.8005\n",
      "\n",
      "Overall CV Pearson Score: 0.8430\n",
      "\n",
      "--- Saving predictions from Model 1 to disk ---\n",
      "Saved 'oof_preds_cos_cite.npy' and 'sub_preds_cos_cite.npy' successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Model 1: Correlation Loss Model ---\n",
    "print(\"\\n--- Training Model 1 (Cosine Similarity Loss) ---\")\n",
    "\n",
    "set_seed(1024)\n",
    "MODEL1_PARAMS = {\n",
    "    'num_features': train_cite_X.shape[1],\n",
    "    'num_targets': 140,\n",
    "    'seq_len': 16,\n",
    "    'd_model': 256,\n",
    "    'nhead': 16,\n",
    "    'num_layers': 3,\n",
    "    'dim_feedforward': 512,\n",
    "    'dropout': 0.1,\n",
    "}\n",
    "\n",
    "TRAIN1_PARAMS = {\n",
    "    'batch_size': 512,\n",
    "    'epochs': 30, \n",
    "    'lr': 1e-3,\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds_cos, sub_preds_cos = train_and_evaluate(\n",
    "    TabularTransformer,\n",
    "    train_cite_X,\n",
    "    train_cite_y,\n",
    "    test_cite_X,\n",
    "    folds,\n",
    "    MODEL1_PARAMS,\n",
    "    TRAIN1_PARAMS,\n",
    "    cosine_similarity_loss\n",
    ")\n",
    "print(\"\\n--- Saving predictions from Model 1 to disk ---\")\n",
    "np.save('oof_preds_cos_cite.npy', oof_preds_cos)\n",
    "np.save('sub_preds_cos_cite.npy', sub_preds_cos)\n",
    "print(\"Saved 'oof_preds_cos_cite.npy' and 'sub_preds_cos_cite.npy' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9622b770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Model 2 (MSE Loss with Z-scored Targets) ---\n",
      "\n",
      "===== Fold 1 =====\n",
      "Epoch 10/30, Val Loss: 0.3461\n",
      "Epoch 00017: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 20/30, Val Loss: 0.3460\n",
      "Epoch 00025: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 30/30, Val Loss: 0.3460\n",
      "\n",
      "===== Fold 2 =====\n",
      "Epoch 10/30, Val Loss: 0.3470\n",
      "Epoch 00016: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 20/30, Val Loss: 0.3468\n",
      "Epoch 00024: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 30/30, Val Loss: 0.3467\n",
      "\n",
      "===== Fold 3 =====\n",
      "Epoch 10/30, Val Loss: 0.3475\n",
      "Epoch 00020: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 20/30, Val Loss: 0.3476\n",
      "Epoch 00027: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 30/30, Val Loss: 0.3474\n",
      "\n",
      "===== Fold 4 =====\n",
      "Epoch 00010: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 10/30, Val Loss: 0.3433\n",
      "Epoch 00017: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 20/30, Val Loss: 0.3431\n",
      "Epoch 00024: reducing learning rate of group 0 to 8.0000e-07.\n",
      "Epoch 30/30, Val Loss: 0.3431\n",
      "\n",
      "===== Fold 5 =====\n",
      "Epoch 10/30, Val Loss: 0.3457\n",
      "Epoch 00020: reducing learning rate of group 0 to 8.0000e-05.\n",
      "Epoch 20/30, Val Loss: 0.3456\n",
      "Epoch 00028: reducing learning rate of group 0 to 8.0000e-06.\n",
      "Epoch 30/30, Val Loss: 0.3455\n",
      "\n",
      "Overall CV Pearson Score: 0.8089\n",
      "\n",
      "--- Saving predictions from Model 1 to disk ---\n",
      "Saved 'oof_preds_cos_cite.npy' and 'sub_preds_cos_cite.npy' successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- Model 2: MSE Loss Model (Target Z-score Normalization) ---\n",
    "print(\"\\n--- Training Model 2 (MSE Loss with Z-scored Targets) ---\")\n",
    "\n",
    "# Key step: Z-score normalize the target y\n",
    "train_cite_y_zscored = zscore(train_cite_y)\n",
    "\n",
    "set_seed(2048)\n",
    "MODEL2_PARAMS = {\n",
    "    'num_features': train_cite_X.shape[1],\n",
    "    'num_targets': 140,\n",
    "    'seq_len': 20, # Try different sequence lengths\n",
    "    'd_model': 240, # d_model must be divisible by nhead\n",
    "    'nhead': 16,\n",
    "    'num_layers': 4,\n",
    "    'dim_feedforward': 600,\n",
    "    'dropout': 0.15,\n",
    "}\n",
    "\n",
    "TRAIN2_PARAMS = {\n",
    "    'batch_size': 512,\n",
    "    'epochs': 30,\n",
    "    'lr': 8e-4,\n",
    "}\n",
    "\n",
    "folds = KFold(n_splits=5, shuffle=True, random_state=1337)\n",
    "\n",
    "oof_preds_mse, sub_preds_mse = train_and_evaluate(\n",
    "    TabularTransformer,\n",
    "    train_cite_X,\n",
    "    train_cite_y_zscored,\n",
    "    test_cite_X,\n",
    "    folds,\n",
    "    MODEL2_PARAMS,\n",
    "    TRAIN2_PARAMS,\n",
    "    nn.MSELoss() # Use standard MSE loss\n",
    ")\n",
    "print(\"\\n--- Saving predictions from Model 1 to disk ---\")\n",
    "np.save('oof_preds_cos_cite.npy', oof_preds_cos)\n",
    "np.save('sub_preds_cos_cite.npy', sub_preds_cos)\n",
    "print(\"Saved 'oof_preds_cos_cite.npy' and 'sub_preds_cos_cite.npy' successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch201-py39-cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
